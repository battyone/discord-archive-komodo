AuthorID;Author;Date;Content;Attachments;Reactions;
"450158653819846656";"nasgifs#0000";"02-Jun-18 01:47 AM";"Kmdbot tip maegus 1";"";"";
"450158653819846656";"nasgifs#0000";"02-Jun-18 01:48 AM";"Kmdbot balance";"";"";
"450158653819846656";"nasgifs#0000";"02-Jun-18 01:50 AM";"Kmdbot tip maegus 1.0";"";"";
"450158653819846656";"maegus#0000";"02-Jun-18 01:51 AM";"Do .9999";"";"";
"450158653819846656";"imylomylo#0000";"02-Jun-18 03:25 AM";"smk762 what a find.  i'll try it out";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:25 AM";"üëç";"";"";
"450158653819846656";"imylomylo#0000";"02-Jun-18 03:26 AM";"if you want to load test AWS....leave a block notify script in your config when leave komodod off for a day üò¨";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:27 AM";"done a bit of digging after serverless errored. Lambda functions might also allow some aggregation, and maybe a subsequent socket emission of thin data.";"";"";
"450158653819846656";"imylomylo#0000";"02-Jun-18 03:28 AM";"""socket emission""?  as in call out to another webservice?";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:29 AM";"seems dynamodb doesn't like too much query, so looking to avoid it by feeding incoming stream events into agreagtion script, which can periodically send aggregate data";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:30 AM";"socket-io via nodejs. it's what I was using for the explorer based data gathering.";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:31 AM";"might not be necessary though, just I already got spare code for it.";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:36 AM";"I got a good story of forgetting about aws. During last round of scaletesting, I setup kinesis, and wandered through aws reading up on things. Somehow, I setup a whole bunch of sharding while looking into capacity and cost. Can't remember hitting ""buy"" or ""apply"" though. The instance they offered with the Alooma trial was using amazon linux as the OS, which didn't play nice when I tried to build komodo, so I logged out and went back to doing work on the collector hosted with vultr.";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:37 AM";"About a week later, went back to aws to see if I could link up.";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:37 AM";"saw this";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:38 AM";"image.png";"https://cdn.discordapp.com/attachments/449949868904022036/452314945556578314/image.png";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:39 AM";"fortunately support gave me a mulligan cos none of the tasked resources had been used.";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:40 AM";"been very careful in aws ever since";"";"";
"450158653819846656";"imylomylo#0000";"02-Jun-18 03:41 AM";"wow - yeah aws has stung me for a weekend of 2xlarge instance running once, $50.  definitely need to üëÄ";"";"";
"450158653819846656";"imylomylo#0000";"02-Jun-18 03:42 AM";"buy a lottery ticket, or play https://stellarball.com/en.php üòÑ";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:44 AM";"I cant play lotto, call it the hope tax. only value it has is the hours you sit with a ticket imagining the win, and the devestating realisation after the draw that I""m due back at work tommorow.";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:44 AM";"That, and I suspect that level of unearner wealth would see me dead within a week";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 03:47 AM";"https://twitter.com/MyWickedUnicorn/status/1002577879660670976";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 04:04 AM";"Work in progress, but here's how I pictured the flow";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 04:04 AM";"image.png";"https://cdn.discordapp.com/attachments/449949868904022036/452321593289605133/image.png";"";
"450158653819846656";"daxirre#0000";"02-Jun-18 06:18 AM";"Hey guys how are those Amazon servers for getting good cpu power?";"";"";
"450158653819846656";"ballwood#0000";"02-Jun-18 09:14 AM";"smk762 have you guys looked at pushing the data to elasticseach? kv store so it‚Äôs like dynamo with better timestamp index support. you can then aggregate over whatever else json you submit. kibana on frontend will give you your visualizations";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 09:16 AM";"looks interesting, cheers :thumbsup_all:";"";"";
"450158653819846656";"smk762#0000";"02-Jun-18 09:27 AM";"updated vis start chart";"https://cdn.discordapp.com/attachments/449949868904022036/452402951768768512/updated_vis_start_chart.png";"";
"450158653819846656";"daxirre#0000";"02-Jun-18 09:31 AM";"Nice explanation";"";"";
"450158653819846656";"daxirre#0000";"02-Jun-18 09:32 AM";"Haha üòÜ";"";"";
"450158653819846656";"blackjok3r#0000";"02-Jun-18 12:23 PM";"whats the system load with 64 chains?";"";"";
"450158653819846656";"blackjok3r#0000";"02-Jun-18 12:23 PM";"and can you run this command `netstat -an | grep TIME_WAIT | wc -l`";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:25 PM";"Screenshot from 2018-06-02 13-24-45.png";"https://cdn.discordapp.com/attachments/449949868904022036/452447581151428608/Screenshot_from_2018-06-02_13-24-45.png";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:25 PM";"iguana using only about 9GB of RAM";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:25 PM";"```
$ netstat -an | grep TIME_WAIT | wc -l
2287
```
blackjok3r ^^";"";"";
"450158653819846656";"blackjok3r#0000";"02-Jun-18 12:25 PM";"sweet thats nothing";"";"";
"450158653819846656";"blackjok3r#0000";"02-Jun-18 12:25 PM";"üòÑ";"";"";
"450158653819846656";"blackjok3r#0000";"02-Jun-18 12:25 PM";"can easlly go more chains. üôÇ";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:26 PM";"yes";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:26 PM";"i even tested notarization only with 2 CPU threads for these 64 chains couple of days ago. it went smooth";"";"";
"450158653819846656";"blackjok3r#0000";"02-Jun-18 12:30 PM";"There is almost no situation where all chains will have full blocks, like we are doing with these chains. So a single server may be able to notarize 256 chains even. The only thing is starting that many chains at once, requires the system to be firewalled otherwise you get port conflicts. This would be a PITA in a real world situation. Managing notary node having to do that would be annoying.";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:51 PM";"though, i didn't have any blocks synced";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:51 PM";"with these ccid and MoMoM printouts, we almost don't see normal notarization printouts. and, they run very fast üòõ";"";"";
"450158653819846656";"blackjok3r#0000";"02-Jun-18 12:51 PM";"It becomes a problem only when chains you first start have TCP connections, and use up ports";"";"";
"450158653819846656";"blackjok3r#0000";"02-Jun-18 12:51 PM";"yeah 64 chains must be intense on the prints lol";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:51 PM";"yes, no notarizations yet on those chains";"";"";
"450158653819846656";"blackjok3r#0000";"02-Jun-18 12:51 PM";"few 0's in there? is that just because not enough notarizations on those chains I guess?";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:52 PM";"i mean, since i started about 30 minutes ago. it can take some time for all chains to start notarizing after iguana started";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:52 PM";"12 chains remaining";"";"";
"450158653819846656";"blackjok3r#0000";"02-Jun-18 12:52 PM";"For sure. We have already found and fixed a lot of bottlenecks.";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:52 PM";"this scaletest will help mainnet notary nodes as well";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:52 PM";"yep";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 12:58 PM";"9 chains remaining now";"";"";
"450158653819846656";"nasgifs#0000";"02-Jun-18 01:15 PM";"Kmdbot tip maegus 0.9999";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 03:50 PM";"1 chain left, 63 notarizing consistently";"";"";
"450158653819846656";"libscott#0000";"02-Jun-18 04:10 PM";"Not seeing notarisations for TXSCL";"";"";
"450158653819846656";"libscott#0000";"02-Jun-18 04:10 PM";"sorry, dont mean to sound ungrateful.. üòÜ";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 04:11 PM";"yes, just funded KMD on one node. noticed that it ran out of KMD 20 minutes ago";"";"";
"450158653819846656";"libscott#0000";"02-Jun-18 04:11 PM";"ooo. so we should be seeing one soon";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 04:11 PM";"TXSCL didn't have notarization for last 15 hours.";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 04:11 PM";"other chains has notarization";"";"";
"450158653819846656";"libscott#0000";"02-Jun-18 04:12 PM";"yea, there were 2200 notarisations since the last one for TXSCL üôÇ";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 04:12 PM";"latest ones were happened 19 minutes ago";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 04:13 PM";"notarization started again";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 05:59 PM";"TXSCL has notarization";"";"";
"450158653819846656";"libscott#0000";"02-Jun-18 06:52 PM";"i see it!";"";"";
"450158653819846656";"libscott#0000";"02-Jun-18 10:15 PM";"anyone got some TXSCL? I deleted mine by accident üòï";"";"";
"450158653819846656";"libscott#0000";"02-Jun-18 10:15 PM";"RGyP6KXDkw7cmF7RjoUmBeoRUh5mCRr4jk";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 10:17 PM";"sending";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 10:18 PM";"e190146defdc0221de29f896eeb9603d3fb47d03b97ceb9a438804f1bbbbc673";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 10:18 PM";"sent";"";"";
"450158653819846656";"libscott#0000";"02-Jun-18 10:18 PM";"thanks! üôÇ";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 10:50 PM";"The MoMoM list is getting bigger";"";"";
"450158653819846656";"libscott#0000";"02-Jun-18 11:00 PM";"üòÆ what's that";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 11:04 PM";"those big list is being printed now";"";"";
"450158653819846656";"shossain#0000";"02-Jun-18 11:07 PM";"all MoMoMs?";"";"";
"450158653819846656";"libscott#0000";"03-Jun-18 12:43 AM";"no, they're moms";"";"";
"450158653819846656";"libscott#0000";"03-Jun-18 12:43 AM";"the leaf nodes of the MoMoM";"";"";
"450158653819846656";"blackjok3r#0000";"03-Jun-18 07:55 AM";"I have modified my docker images so that both tests are run in one go. it first blasts 1 payment TX's then blasts 100 payments. I just need to calibrate how long to wait between these so that nothing overlaps.";"";"";
"450158653819846656";"blackjok3r#0000";"03-Jun-18 07:56 AM";"I am running out of improvements to make, and will soon need AWS access so we can figure out the shared storage and how much vCPU is required for each chain.";"";"";
"450158653819846656";"jl777#0000";"03-Jun-18 07:58 AM";"time for the AWS talk so we can get the ball rolling on the world record test";"";"";
"450158653819846656";"blackjok3r#0000";"03-Jun-18 08:06 AM";"I need to make sure its 100% working for a few tests but its safe to say I think its good. By the end of tomorrow it will defiantly be finished. There is only the stats end left now. Currently all the data is pushed to that AWS dynamoDB its just up to @Mylo and smk762 to work out what to do with it.";"";"";
"450158653819846656";"jl777#0000";"03-Jun-18 08:08 AM";"have we verified the data is good";"";"";
"450158653819846656";"blackjok3r#0000";"03-Jun-18 08:18 AM";"It should be fine. All i am doing is taking the length of the .tx in `getblock` rather than pushing all the transaction hashes. the code is all commented if anyone wants to check it. 
its located here: https://github.com/blackjok3rtt/scaletest_containers/tree/1payment2";"";"";
"450158653819846656";"shossain#0000";"03-Jun-18 07:29 PM";"1500 notarizations in 5 hours. every hour doing about 300 and 5 each minute";"";"";
"450158653819846656";"manfromaus#0000";"04-Jun-18 12:20 AM";"Someone mentioned wanting numbers around BTC average payments/tx - this article should give some useful info: https://coinmetrics.io/batching/";"";"";
"450158653819846656";"manfromaus#0000";"04-Jun-18 12:21 AM";"for the ""real world"" comparative test";"";"";
"450158653819846656";"smk762#0000";"04-Jun-18 12:59 AM";"updated star chart";"https://cdn.discordapp.com/attachments/449949868904022036/452999798539157526/updated_star_chart.png";"";
"450158653819846656";"smk762#0000";"04-Jun-18 01:00 AM";"updated star chart";"https://cdn.discordapp.com/attachments/449949868904022036/453000077380681739/updated_star_chart.png";"";
"450158653819846656";"emmanux#0000";"04-Jun-18 01:01 AM";"lol";"";"";
"450158653819846656";"jl777#0000";"04-Jun-18 04:21 AM";"batching: ""Batching accounts for roughly 12% of all transactions, 40% of all outputs, and 30‚Äì60% of all raw BTC output value. Not bad.""";"";"";
"450158653819846656";"jl777#0000";"04-Jun-18 04:22 AM";"maybe we need to do more education about the benefits of batching. For most businesses that are not time sensitive in payments, they can batch 100 tx into a single one and save 90% of space";"";"";
"450158653819846656";"smk762#0000";"04-Jun-18 04:29 AM";"payroll is common and an obvious use case.";"";"";
"450158653819846656";"jl777#0000";"04-Jun-18 04:31 AM";"yes for that it can even become a 1000 payment tx";"";"";
"450158653819846656";"jl777#0000";"04-Jun-18 04:32 AM";"but also payables, i can see an automate payments batching system that companies use that combines multisig approvals to save costs for BTC txfees, and once this is available it would work with any bitcoin protocol coin";"";"";
"450158653819846656";"blackjok3r#0000";"04-Jun-18 04:37 AM";"Yes. We strongly encourage any analysts, investors, journalists, and developers to look past mere transaction count from now on. *The default measure of Bitcoin‚Äôs performance should be ‚Äúpayments per day‚Äù rather than transaction count.* This also makes Bitcoin more comparable with other UTXO chains. They generally have significantly variable payments-per-transaction ratios, so just using payments standardizes that. (Stay tuned: Coinmetrics will be rolling out tools to facilitate this very soon.)";"";"";
"450158653819846656";"blackjok3r#0000";"04-Jun-18 04:37 AM";"ü§î";"";"";
"450158653819846656";"imylomylo#0000";"04-Jun-18 05:25 AM";"ok.  thanks for req";"";"";
"450158653819846656";"smk762#0000";"04-Jun-18 05:34 AM";"percentage levies on payment processing (visa, mastercard), plus potential interest on the debt incurred, and inflation caused, all while giving up trade privacy and being subject to profiling based on purchase item/time/locatation
vs.
low flat rate fee for batch payment processing + opt-in methodology for sharing private metadata";"";"";
