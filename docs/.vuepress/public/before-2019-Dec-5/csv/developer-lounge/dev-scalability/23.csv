AuthorID;Author;Date;Content;Attachments;Reactions;
"455846022602555392";"John the Cashist#7046";"07-Jul-18 09:53 PM";"We need a crypto olympics ü§îü§î";"";"";
"456226577798135808";"Deleted User#0000";"08-Jul-18 09:27 PM";"didn't check this chan for a few time, in fact the script can be ran on any server / instance, the only tricky part would be just to extract each chain own table, and just run the counting for the global table. Assuming it has been done with hundreds of chains without harm, as long as we're able to get each chain's table, it's only a few commands to import em on same db instance";"";"";
"456226577798135808";"Deleted User#0000";"08-Jul-18 09:27 PM";"and just launch the ""globalizing"" method";"";"";
"456226577798135808";"Deleted User#0000";"08-Jul-18 09:34 PM";"the db for tech users just wouldn't be a proof, I think, launching the chains and verifying blocks would be a solution, to see peaks, it can also be combined with the db/webinterface to query automagically the corresponding blocks (for example just a little script with the db at a timestamp could return the exact block involved in the peak, then could be used in a loop to check on every chain the number of transactions)";"";"";
"456226577798135808";"Deleted User#0000";"08-Jul-18 09:36 PM";"that PoV would require being on CLI. The only method for users not using CLI to check it would be explorers, but I think it was said that it was a pain in term of hardware required. Just in case it is needed, it's the same as the CLI : It's totally possible to ""link"" the peak to each specific explorer URL to count via GUI";"";"";
"136534544470900736";"benohanlon#1119";"09-Jul-18 10:17 AM";"thanks @sœÜldat I'm on it @jl777c now";"";"";
"136534544470900736";"benohanlon#1119";"09-Jul-18 10:18 AM";"- posting in their telegram and emailing them";"";"shexy (2)";
"232679450301431808";"blackjok3r#3181";"10-Jul-18 08:21 AM";"When you fire up AWS to run a scale test, and its still not working üò´";"";"";
"371114647052615681";"Mylo#8306";"10-Jul-18 08:36 AM";"d'oh.  which part isn't working?";"";"";
"232679450301431808";"blackjok3r#3181";"10-Jul-18 08:45 AM";"We have EC2, but its locked down to tiny instances. Cant launch anything meaningful in size. I applied for increase, but if past encounters with support are anything to go by, it could take a while.";"";"";
"232679450301431808";"blackjok3r#3181";"10-Jul-18 08:46 AM";"EG I can apply for an m5.large instance... but I need an m5.x16large lol...";"";"";
"232679450301431808";"blackjok3r#3181";"10-Jul-18 08:46 AM";"so do we need to increase limits one teir at a time? or will they just upgrade directly to the top one is the question lol.";"";"";
"232679450301431808";"blackjok3r#3181";"10-Jul-18 09:05 AM";"Lucky we still have @Mylo 's account. üòÑ";"";"";
"232679450301431808";"blackjok3r#3181";"10-Jul-18 09:06 AM";"I am rebuilding my docker image using komodo DEV branch, rather than momo as MoMoM was merged into that a while back.";"";"";
"232679450301431808";"blackjok3r#3181";"10-Jul-18 09:06 AM";"ü§û everything is working in sync now.";"";"üí™ (1)";
"400784276620050433";"anonanon#1085";"11-Jul-18 11:27 PM";"Say I'm concerned the recent 51% attacks on zencash and BTG will be repeated on lots of cryptocurrencies that share common mining algorithms. I figure Komodo's dPOW is intended to protect against that, but can anyone tell me what it would look like if someone rented 51% of the KMD hashpower and tried to perform a double spend or otherwise mess with the chain?";"";"";
"455741312273219595";"jl777c#5810";"11-Jul-18 11:28 PM";"notaries mine 75% of blocks";"";"";
"455741312273219595";"jl777c#5810";"11-Jul-18 11:28 PM";"so 51% of 25% is about 13% of effective hashrate";"";"";
"455741312273219595";"jl777c#5810";"11-Jul-18 11:29 PM";"KMD has some unique defenses against 51%";"";"";
"455741312273219595";"jl777c#5810";"11-Jul-18 11:29 PM";"in addition to dPoW notarization";"";"";
"455741312273219595";"jl777c#5810";"11-Jul-18 11:29 PM";"which stops cold any attempt at double spends (up to the notarized height)";"";"";
"455741312273219595";"jl777c#5810";"11-Jul-18 11:30 PM";"in other words, you cant rent 51% of KMD hashpower as 75% of the effective hashrate is with the notaries";"";"";
"400784276620050433";"anonanon#1085";"11-Jul-18 11:30 PM";"Right, nice. Ty!";"";"";
"400784276620050433";"anonanon#1085";"11-Jul-18 11:40 PM";"Oh I'm not sure how I ended up in scaling-test. My bad y'all.";"";"";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 09:58 AM";"OK guys...";"";"";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 09:58 AM";"http://cryptocartography.io/txscl_vis/";"";"";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 10:01 AM";"The TX and Payments per second reported live and confirmed to be 90% or more accurate. No mean feat I assure you. This is still only 64 chains and will be one of the last ""real"" tests we run, before moving to a ""block simulator"" to test the site at very large scales of chains. 

The TxBlaster itself is proving very reliable and there is no longer a reason to suck up AWS credits to run full blockchain tests. So we will move to a randomly generated stats model, and keep doubling the number of chains until the site breaks. We can separate out the back end of the website into multiple streams to scale to any number. Just like the TxBlaster's which can scale to infinity given enough CPU's .";"";"";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 10:03 AM";"If anyone is around to watch the site in the next hour or so please let us know if there is anything weird going on. 
known issues:
1) the Blocks per minute number jumps around a bit, we might need to apply a soothing functon to this data, blocks come in chunks, and then breaks with no blocks. Its just how blockchains work.
2) Refreshing the website, which moves it to 'historical data' causes the reported data to be incorrect. This is being worked on, and is not anything major. Live data is correct.";"";"";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 10:04 AM";"https://komodostats.com/scaling/index2.php?all";"";"";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 10:05 AM";"That is another data source from out test last night that we used to verify the site is right. Approx 9000 - 10,000 TX/s peak and 7500 Tx/s sustained.
61,000 payments per second. peak... 
55,000 PPS sustained.";"";"üëè (2)";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 10:07 AM";"üöÄ";"";"";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 12:56 PM";"https://youtu.be/kXk7Y3Si140";"";"";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 12:58 PM";"Short summery of the last test. The https://komodostats.com/scaling/index2.php will be updated with the results of this test when webworker is available.";"";"";
"456226577798135808";"Deleted User#0000";"12-Jul-18 01:19 PM";"Mmm grass fed beef üç¥";"";"";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 01:23 PM";"lol link smk762 gave me yesterday... forgot it was open lol";"";"";
"232679450301431808";"blackjok3r#3181";"12-Jul-18 01:24 PM";"I mostly did the video to record the data, as the site breaks when you refresh it atm.";"";"";
"232679450301431808";"blackjok3r#3181";"13-Jul-18 02:47 AM";"@webworker01 any chance you can change the SMA to 5 mins... as thats what smk's site uses... that might give me a better way to compare data.";"";"";
"232679450301431808";"blackjok3r#3181";"13-Jul-18 02:47 AM";"I think te raw data is much the same, but the way its smoothed out is diffrent so gives diffrent numbers";"";"";
"232679450301431808";"blackjok3r#3181";"13-Jul-18 03:35 AM";"yeah thats fine. It keeps both seperated... if you look at the video of the stats site, smk seperates them, with two diffrent graph colours.";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"14-Jul-18 03:22 AM";"Vis history smoothing has been fixed. Had a small issue where some blocks were not being captured on the vis, leading to underreporting of transactions/payments.The fix that made sure no blocks were dropping involved recording values in 5 second increments, with increments aligned to be a unix timestamp which is a factor of 5.
It wasn't really the smoothing algo that was the problem, it was the way history was being recorded and populating the graph.";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"14-Jul-18 03:25 AM";"The fix to make sure all data was being recieved live resulted in blocks that were arriving late being recorded, but created potentially 3 or 4 database entries with the same timestamp. Instead of the sum of these being emitted, only the first recieved was, meaning even though blocks were being recieved and recorded, and reporting true with live data, the history data from after the initial fix wasn't passing all the info on to the highcharts graph.";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"14-Jul-18 03:27 AM";"edits were made last night which ensured all source data recieved and stored in history are passed in full to the graph.";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"14-Jul-18 03:28 AM";"A note has also been added to explain the minor differences between the vis graph and komodostats.com";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"14-Jul-18 03:29 AM";"*Values represented in this visualisation are plotted at 5 second increments, at intervals of 1 and 5 minutes. The finer resolution of graph points per minute, and the slight delay in recieving and processing block data, results in minor variations to values compared to the source block data graph at komodostats.com, whick are plotted at 1 and 5 minute resolution. The source block data has been stored for verification, and is normally plotted at komodostats.com within 24hrs after each test.*
*Comparison between source block data and aggregate data used for this visualisation are reviewed after each test, with validated test periods highlighted with a light green background.*
*Random data which has been used for testing is purged periodically.*";"";"üí™ (4),ü•â (3)";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"14-Jul-18 03:54 AM";"we're now on track to add more chains, and find out where that saturates the database and 5 second increment. From there, we should be able to find out what increment is best for 8192 chains, make some edits to widen it accordingly, and march on to 1million tx üòÉ";"";"üî• (3)";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 03:58 AM";"Will work on a block simulator this afternoon, see if we can fill the DynamoDB with 8192 chains üòÑ";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"14-Jul-18 04:23 AM";"https://i.imgur.com/hscsCQJ.jpg";"";"‚ö° (1)";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 12:14 PM";"https://i.imgur.com/DqrP09z.png";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 12:14 PM";"üòÇ";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 12:15 PM";"Using simulated data rather than actual blockchains, but its working perfectly.";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 12:15 PM";"Does anyone have any requests of a number we should aim for using simulated data? I think maybe we can make it pay everyone in the entire world in 30s without too much effort ü§£";"";"";
"455741312273219595";"jl777c#5810";"14-Jul-18 12:18 PM";"pretty crazy!";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 01:15 PM";"About to start a simulated 4096 chains test. üöÄ";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 01:17 PM";"Simulating works by sending the JSON with a the data of a full block at random intervals between 30-90s to the AWS DynamoDB. Saves a huge amount of CPU's and  storage space... but really is not much different to using a real blockchain to generate the data.";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 01:18 PM";"once the stats are verified to work with X chains, then we are ready. All we need is AWS to allow out accout to launch X vCPU's at once. We can even use multiple regions if need be.";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 01:19 PM";"Or even multiple accounts if it comes down to that.";"";"üëå (1),shexy (1),üëç (1)";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 03:24 PM";"ü§£";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 03:24 PM";"Redlined the speedo";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 03:24 PM";"http://cryptocartography.io/txscl_vis/";"";"";
"232679450301431808";"blackjok3r#3181";"14-Jul-18 03:28 PM";"https://youtu.be/pVEx1Hg_I7I";"";"";
"371114647052615681";"Mylo#8306";"15-Jul-18 12:58 AM";"That's awesome!! So when are we going to be doing some of these crazy tests?  Will it be in the next:
 a) 1 -3 days
 b) 3-5 days
 c) 7-10 days
 d) ask tomorrow

üòÉ";"";"";
"232679450301431808";"blackjok3r#3181";"15-Jul-18 01:23 AM";"That's not known at this stage, I await patchkez to return from vacation to help me with the last of the docker stuff, need to create a kubernetes template, and also Steve has to sort the last things out with AWS, so we have an account big enough to launch the full test. Some time in the next week I will run a full block chain test with real data that aims for 45k tx/s minimum to test out my docker containers running in kubernetes to verify everything works. Also need to get some notary nodes and explorer's for the first 64 chains ready. Maybe I can just run these in AWS as well, but in a different region. Not sure yet.";"";"üëå (1)";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"15-Jul-18 03:36 AM";"Ive added a couple of things to the graph point tooltips - commas for number formatting, active chain count, and data state (live / simulated).";"";"üëç (1)";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"15-Jul-18 03:36 AM";"https://i.imgur.com/vkulxpX.jpg";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"15-Jul-18 04:08 AM";"Is there a plan to host the source data at a publicly available url for validation?  I understand it is a lot of data... but I'd like to add a link for the curious. Maybe a link to contact details to someone which can provide it on request for a specific time period?";"";"";
"232679450301431808";"blackjok3r#3181";"15-Jul-18 04:35 AM";"I think the blockchains will be saved in an AWS S3 data store, however running 8192 deamons to extract it would be difficult. I will work with dwy to adapt his script that dumps the data to a mysql DB. Maybe that can be put out to public for reads?";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"15-Jul-18 05:01 AM";"Third party verification from an independent, trusted person/group is probably most valuable marketing-wise, but opening it to the public also shows good faith. Most wouldn't bother to check, but availability speaks volumes. Not sure what format is best. Database tables are easily populated with whatever, but a full copy of all chains blk.dat etc is closer to source, though a significant amount of data to review. Probably a few random samples would suffice.
The scripts which are used to run the tests being open source also helps, at least for those that understand it. I don't think we'll have issues with burden of proof if the right people with no vested interest confirm it's valididty.";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"15-Jul-18 05:02 AM";"sql dumps are small enough for public consumption and a good indicator of valididty. Official validation would likely want access to more";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"15-Jul-18 05:03 AM";"I think that expanding the notes link on the vis site to be a pop-up onclick with links to sql and all relevant github repos isnt a bad idea";"";"";
"448777271701143562";"smk762 -  dracocanis ominator#7640";"15-Jul-18 05:06 AM";"something like this, if it allows importing blk.dat files, would be handy - http://yogh.io/#block:last";"";"";
"456226577798135808";"Deleted User#0000";"16-Jul-18 10:59 PM";"btw, maybe you can try this to test stuff : https://labs.ovh.com/kubernetes-k8s";"";"";
"456226577798135808";"Deleted User#0000";"16-Jul-18 10:59 PM";"seems they provide alpha phase totally free";"";"";
"456226577798135808";"Deleted User#0000";"16-Jul-18 11:00 PM";"don t know if that's stable, but for testing purposes maybe it's worth trying";"";"";
"406179072922746882";"polycryptoblog#1173";"17-Jul-18 01:48 AM";"3rd party trusted verification you say,  hmm may have the guys for that.";"";"üõ∞ (2)";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 01:52 AM";"Yeah that would a good, if we can set up something like that. We are looking at an estimated 6-7TB of blockchain data generated in the test, but depending on cost, it could be much larger as I would prefer to make the test run longer than that if possible.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 02:17 AM";"Btw, thanks @Deleted User  I signed up for that OVH Alpha test, doubtful they have anything near the needed amount of CPU's but the way the test works, we can spread over loads of providers if need be.";"";"üòÉ (1)";
"371114647052615681";"Mylo#8306";"17-Jul-18 07:46 AM";"";"https://cdn.discordapp.com/attachments/449949868904022036/468685021880778773/Metric_Definition_Proposal.pdf";"";
"371114647052615681";"Mylo#8306";"17-Jul-18 07:48 AM";"This is a PDF print of a google doc shared by hyperledger.
Link from the working group home page (WG Working Documents) here https://wiki.hyperledger.org/groups/pswg/performance-and-scale-wg";"";"";
"371114647052615681";"Mylo#8306";"17-Jul-18 08:04 AM";"From the last test @blackjok3r @webworker01 @smk762 -  dracocanis ominator - is it true to say:
```
 - Live test to optimize using a smaller load
 - 6k tx/s for 15 mins
 - 4k tx/s for 30 mins
 - 2k tx/s for 60 mins
 - details: https://komodostats.com/scaling/index2.php?all
```
?";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:06 AM";"It depends what you are trying to say... I can launch a test with 256 chains that cna run for 24H if thats a better meteric to use?";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:06 AM";"Maybe we should run some smaller scale tests for extended time periods, then a really large amount of chains for short time?";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:07 AM";"then we can prove that its technically possible to sustain very large numbers at scale... Its only a cost factor that limits the time.";"";"";
"371114647052615681";"Mylo#8306";"17-Jul-18 08:07 AM";"Yes, can we run a small scale for 12 hours doing 2k tx/s with stats working the whole time?
What is the cost difference between 2k, 4k, 8k tx/s for the purpose of this test?";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:09 AM";"64 chains seems to cap out at about 7000tx/s of single payment, but sustaining that for long periods is difficult... if you increase it out to 3-4 payment TX's it should be possible to run indefinitely.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:09 AM";"Or I can go back to an earlier iteration, that uses 2 TxBlasters to blast for unlimited time.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:09 AM";"It was just not possible to use that version at very large scale of chains, dude to limits of hardware available.";"";"";
"371114647052615681";"Mylo#8306";"17-Jul-18 08:10 AM";"Is there a bottleneck with the single payment/";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:11 AM";"yes on the transaction generating side... it requires 2 marketmakers/komodods to broadcast enough transactions to fill the mempool.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:11 AM";"When you are using one wallet to simulate over 1000 wallets, its a little hard.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:11 AM";"If we had 1000 wallets to send TX from.... then it would not matter";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:12 AM";"Once again, limited by hardware.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:12 AM";"Short of actually using it for a national currency, its all but impossible to get that many users.";"";"";
"371114647052615681";"Mylo#8306";"17-Jul-18 08:17 AM";"so 64 chains doing single payment ~= 7k tx/s
doing 5 payments  allows for marketmaker to keep up with blasting to fill mempool quick enough by sending transctions containing more bytes but sustainable for longer.  Load testing endurance rather than throughput.
?

Users will be machines and dApps.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:19 AM";"Yes, I have not actually tested a threshold for payments per TX that allows sustained full blocks, but 100 needs nothing, 10 chains will be blasting at any one time over 64 chains. but 1 payment is not really possible to keep mempool saturated. For this I used a mining pause at the start to fill all the mempools up.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:20 AM";"I can definatly do that, although like I said there is an older version, that can sustain single payments, it just needs an extra CPU per chain.";"";"";
"371114647052615681";"Mylo#8306";"17-Jul-18 08:20 AM";"Nah let's not use older version imo.  Endurance is another metric that will need to be known once throughput is grokked.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:21 AM";"Not really, because the transactions being generated in this way do not apply to any real world use... they are a simulation.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:21 AM";"What is being tested is the amount of confirmed TX that is possible.";"";"";
"232679450301431808";"blackjok3r#3181";"17-Jul-18 08:23 AM";"Testing the TxBlaster API over endurance is pointless, and I think this is why James said a 15 minute window of all chains with full blocks was all that was required.. I did have a single chain locally runnign for over 5H with full blocks though, so it can be run for a long time";"";"";
"371114647052615681";"Mylo#8306";"17-Jul-18 08:25 AM";"OK - very specific goal.  Got it.";"";"";
