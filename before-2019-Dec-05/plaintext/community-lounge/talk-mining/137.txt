==============================================================
Guild: Komodo
Channel: talk-mining
Topic: ./komodod -ac_name=ZAWY24 -ac_adaptivepow=1 -ac_supply=1000000 -ac_reward=10000000000 -ac_cc=2 -addnode=5.9.102.210 &

A channel to discuss mining. For useful links and more info click the pinned messages to the right.
Messages: 100
Range: before 05-Dec-19 12:00 AM
==============================================================

[02-Aug-19 12:26 PM] CHMEX#0686
16: bad-txns-premature-spend-of-coinbase. Code:-26
Is the error I get
How long does it take till these coinbase are mature?

[02-Aug-19 12:36 PM] jl777c#5810
maybe it is 100 for KMD

[02-Aug-19 12:36 PM] jl777c#5810
sorry, dont remember

[02-Aug-19 12:39 PM] dukeleto#7326
@CHMEX normal utxos become spendable by the wallet with just 1 confirmation (by default, some RPCs will allow you to spend 0 confirm utxos), but coinbase utxos must become "mature" which means 100 confs

[02-Aug-19 12:40 PM] dukeleto#7326
I don't know of any wallets that prevent a transaction with only 1 conf and require 2 confs, to spend. 2 confs is mostly for exchanges/pools to know that something is notarized

[02-Aug-19 12:40 PM] CHMEX#0686
thank you both! I tried now with 100 and that works. Thank duke for the explenation!

[02-Aug-19 12:40 PM] dukeleto#7326
I think there is an -ac_cbmaturity option now, so you can choose something different than 100 for new chains

[02-Aug-19 12:41 PM] CHMEX#0686
( I'm trying to build a sendaway script for mined blocks which are deleted from the wallet because of cleanwallettransactions

[02-Aug-19 12:41 PM] dukeleto#7326
100 is inherited from BTC and ZEC nor anybody else changed it

[02-Aug-19 12:41 PM] dukeleto#7326
but with dpow, 100 confs is a bit much

[02-Aug-19 12:41 PM] dukeleto#7326
which is why -ac_cbmaturity was born, requested by CG I believe

[02-Aug-19 12:41 PM] CHMEX#0686
nice

[02-Aug-19 12:41 PM] dukeleto#7326
All current prod chains use the default 100

{Reactions}
abusedsmily 

[02-Aug-19 12:43 PM] jl777c#5810
pretty sure most AC dont

[02-Aug-19 12:43 PM] jl777c#5810
only ones with eras

{Reactions}
👆🏽 

[02-Aug-19 12:45 PM] dukeleto#7326
@jl777c interesting, I guess era boundaries make for lots of edge cases. I know Hush (which uses eras) does have the default 100, like the old Hush codebase

[02-Aug-19 04:23 PM] SHossain#0007
@Alright and Anyone wants to abuse this chain? This version is improved and more stable than the initial version. Diff adjustment happens in less than 15 mins and drops pretty fast. We no longer need to wait for 10 hours for the diff to be CPU minable level. If you want to test the daemon or your own chain/pool, use `nSPV` branch.
```
./komodod -ac_name=ZAWY5 -ac_adaptivepow=1 -ac_supply=1000000 -ac_reward=10000000000 -ac_cc=2 -addnode=178.159.11.114 &
```
**Pool:** 
Port 3033 maxdiff set to 16 for normal ASIC.
```
stratum+tcp://178.159.11.114:3033
```
Port 3034 maxdiff set to 292 for NH
```
stratum+tcp://178.159.11.114:3034
```
Any hash is welcome for this test.

[02-Aug-19 04:35 PM] SHossain#0007
Nice Hash port added `3034`
Stats: http://178.159.11.114:8080/stats

[02-Aug-19 04:39 PM] ComputerGenie 👴🏼📢#7004
_will try to find time to play with it, but it's his wife's birthday weekend so he can't make promises_

[02-Aug-19 04:40 PM] SHossain#0007
poinit a megasol or two in the pool :troll:

[02-Aug-19 04:45 PM] jl777c#5810
try to abuse it by mining when diff is low and switching out when diff is high

[02-Aug-19 04:45 PM] jl777c#5810
see how fast it adjusts, have no regard to the single CPU miner that will keep the chain going no matter how high the hashrate goes

[02-Aug-19 06:41 PM] dukeleto#7326
https://twitter.com/dukeleto/status/1157360320790355968

{Embed}
Duke Leto (@dukeleto)
https://twitter.com/dukeleto/status/1157360320790355968
The #ZcashDevfund is a corrupt bailout for a financially inept Zcash Company (ECC), whose greatest product-market fit is losing money. 
$ZEC miners, revolt! You do not need to support this treachery!
https://pbs.twimg.com/media/EA_FDiiWwAAVNaz.jpg:large
Twitter

[02-Aug-19 06:41 PM] dukeleto#7326
#ZcashDevFund hashtag on twitter is blowin' up

[02-Aug-19 06:41 PM] dukeleto#7326
everybody can't wait to give more of their mining resources to a corrupt company

[02-Aug-19 07:30 PM] phm87#7395
@dukeleto  if a zcash miner is profitable mining ZEC, does he revolt if he mines the most profitable coin (whatever if it is ZEC or not) and ask to be paid in anything except ZEC ? Mining rewards will be sold on exchange (dump of ZEC) and buy orders will be added on the payment coin
Because if many ZEC miners begin to mine Kmd or Hush, Kmd/hush nethash will increase and profit will decrease

[02-Aug-19 07:35 PM] AkiraX#8269
Surely someone's gotta buy

[02-Aug-19 07:40 PM] Alright#0419
@dukeleto https://i0.kym-cdn.com/photos/images/newsfeed/000/353/279/e31.jpg

{Embed}
https://i0.kym-cdn.com/photos/images/newsfeed/000/353/279/e31.jpg
https://i0.kym-cdn.com/photos/images/newsfeed/000/353/279/e31.jpg

{Reactions}
troll_king 

[02-Aug-19 07:43 PM] zawy#5864
zooko is a meat head

[02-Aug-19 07:44 PM] Alright#0419
I can understand why they want to do it 🤑 , but I don't understand why more people aren't upset that they reneged on previous promises

[02-Aug-19 07:44 PM] zawy#5864
I think it's apathy

[02-Aug-19 07:44 PM] Alright#0419
as jl pointed out couple days ago, it seems like this was always planned

[02-Aug-19 07:44 PM] zawy#5864
people are not even interested in Zcash anymore

[02-Aug-19 07:45 PM] Alright#0419
why not have static 10% always?

[02-Aug-19 07:45 PM] Alright#0419
I am paranoid that they are compromised by special interests 😅

[02-Aug-19 07:45 PM] Alright#0419
👽 👻

[02-Aug-19 07:45 PM] Alright#0419
There is a back door of sorts that can kill the network that they're well aware of

[02-Aug-19 07:45 PM] Alright#0419
and put there on purpose

[02-Aug-19 07:46 PM] Alright#0419
and used crazy justifications when originally implemented

[02-Aug-19 07:47 PM] Alright#0419
https://github.com/zcash/zcash/blob/c68511b8760f2f699984c2e6db710957848f8e56/src/main.cpp#L3269

{Embed}
https://github.com/zcash/zcash/blob/c68511b8760f2f699984c2e6db710957848f8e56/src/main.cpp
zcash/zcash
Zcash - Internet Money. Contribute to zcash/zcash development by creating an account on GitHub.
https://avatars0.githubusercontent.com/u/16122764?s=400&v=4

[02-Aug-19 07:47 PM] Alright#0419
>99 block reorg can quite literally shut down the network

[02-Aug-19 07:47 PM] Alright#0419
and keep it shut down until centralized entity fixes it

[02-Aug-19 07:48 PM] zawy#5864
dev fees like this should be time-locked as if they were founders in a public company.....because they are.  Insiders are supposed to publish their holdings and what they sell, and not be able to sell in under 2 years.

[02-Aug-19 07:48 PM] Alright#0419
"for your safety"

[02-Aug-19 07:59 PM] ComputerGenie 👴🏼📢#7004
someone needs to learn the difference between `!` and `?`

[02-Aug-19 07:59 PM] ComputerGenie 👴🏼📢#7004
@dukeleto

[02-Aug-19 10:44 PM] zawy#5864
@jl777c  Taking 150 blocks to rise 10,000x is too long when it can recover after an attack in maybe 30 blocks.  The following rises 1/2 way to 10,000x in 15 blocks, replacing the trigger with  3 simple moving averages with windows N=4, 7, and 12. 
```
bnTarget =min(bnTarget, sum(past 4 targets)*T/(nTime-B[-4].nTime)/5, sum(past7 targets)*T/(nTime-B[-7].nTime)/3, sum(past12 targets)*T/(nTime-B[-12].nTime)/2);
```
Difficulty algorithms have a choice between speed of reponse and stability.   When triggers are added to a moving average of any sort (Digishield, SMA, LWMA, or EMA) as a way to deal with sudden large changes in hashrate, you have to choose between bad oscillations if the miners come and go (causing long delays) and  blocks being issued too fast or too slow. If the rise and fall are done "symmetrically" the solve times come out good and the worst case is oscillations with long delays. I always do symmetrical algos. If you rise faster than you fall, too few blocks are emitted if the attackers are persistent.  Falling faster than rising leads to too many blocks.  We should attempt faster rises than falls, but also avoid the long delays.   I have done mild versions of what we're attempting here and in about 25% of the cases it makes things noticeably worse.  Each time I think "hmmm, this looks like it will really work" to be disappointed later.  Miners find that sweet spot.   I can't predict what this is going to do.  I don't know if it can be said to rise faster than it falls.  There is the risk of trying to prevent problems from a few coins that have >100x attacks and making things worse for a lot more coins with <10x problems.  This algo's behavior depends completely on miner behavior.    You won't see a difference from digishield if the hashrate is relatively stable, except for about 5 spikes per day that  last 1 or 2 blocks each.  About once a week you'll see a 4x increase for 1 block.

[02-Aug-19 11:01 PM] blackjok3r#3181
if blocks are going too fast why not reduce the block reward?

[02-Aug-19 11:04 PM] blackjok3r#3181
if the difficulty increases sharply pay less

[02-Aug-19 11:21 PM] zawy#5864
In terms of miner motivation, an increase in difficulty is supposed to be exactly equal to the same percent decrease in reward.  Miner motivation is what difficulty targets and miner motivation is proportional to reward/difficulty.  We could change reward, but if we change reward and difficulty at the same time, the adjustment factor determined by the algorithm must  be  equal  SQRT(difficulty/reward).    Changing only reward/time would target a difficulty in the way changing difficulty targets a reward/time.  Is it reasonable to target a difficulty?   If we target difficulty and knew what moore's law is going to do, can it achieve a constant value coin?    If we're not attempting something special like this, then the reward would have to be like a capacitor or an escrow account that stores up excess reward when it can, then releases it later to keep difficulty more stable.  And if the estimates are wrong, and you want to follow a certain emission rate, then it would have to revert to difficulty when the artificial bank account runs out of the reward savings.

{Reactions}
👍 

[03-Aug-19 12:32 AM] zawy#5864
We're changing the difficulty during the block.  I guess we could change the reward based on when the block is submitted.  For example if it's submitted at 6 seconds, then reward is 1/10. If it's submitted at 120 seconds then reward is 2x.  ( We're also making thenodes keep precise time which means the miners can't cheat with a forward stamp. ) The linear function  may not be good.  There's a function I developed that may relieve some of the miners' burden of trying to decide  if and when to mine.   It would be about 1/2 reward for fast solves and 4x reward for some of the late solves, being symmetrical in a way that follows the poisson probabilities.  With or without that,  I think it can be done by making the difficulty window > 2x longer than the reward averaging window, if you want reward to smooth out difficulty.  But I don't know offhand how to do that, keeping the SQRTrequirement in mind.

[03-Aug-19 12:33 AM] dukeleto#7326
@zawy my hunch is that ` Miners find that sweet spot. ` is related to finding resonances/eigenmodes/zeros of the function which describes the DAA

[03-Aug-19 12:34 AM] dukeleto#7326
all of this reminds me of my studies of differential equations, with forcing functions

[03-Aug-19 01:04 AM] zawy#5864
The worst problems occur when a big miner wants  the coin 50% more than anyone else,  constantly drives it up 50%. then leaves and no one mines it.  This is seen by tight sawtooth patterns (block number on x-axis, not time).   Since we're changing the difficulty during the block  it's not going to get stuck.  The only way to stop too many blocks from being found is to increase difficulty quickly....very quickly if  big miners are going to come back as soon as it drops back down. Since we can change difficulty during the block, it might be possible to prevent all problems. And maybe reward can do it.   For example, if last block was found in 20 s then maybe difficulty increase some and reward increases from 0 to 1x  at 60+20, and then difficulty drop if it takes longer to solve.  Then if it took  100 seconds the reward in next block will increase from 0 to 1x at 60-20 second while keeping the  difficulty half way between the avg and what it was at when the previous block was found.  Getting this working requires some thought.

[03-Aug-19 01:07 AM] blackjok3r#3181
How accurate can the timestamps be?

[03-Aug-19 01:17 AM] zawy#5864
I've told them I do not see a problem with the FTL being 4 seconds and changing Zcash's MTP from 11  to 1.  By doing this, everyblock is required to be timestamped 1 second after the previous one.  By tighting the allow rand of tmiestamps forwarding only 4 seconds (the only way to profit a little more) they can't profit hardly anything and risk nodes with slight time errors rejecting them.  BTW, this means if median node peer time is > 2 seconds off from a node's system tiem, the node should revert to system time. system time is the ultimate independent oracle that secures the blockchain against miners trying to increase block emission.  No central server for time should be used.

[03-Aug-19 01:17 AM] zawy#5864
I mean, yes NTP is probably good and can keep everyone accurate, but technically it is a centralization that should be avoided.  Ideally everyone uses a variety of accurate sources

[03-Aug-19 01:18 AM] blackjok3r#3181
If its able to be that tight, why not just not allow fast blocks all together ?

[03-Aug-19 01:19 AM] zawy#5864
JL asked me about that today..  Miners will end up submitting blocks at the same time, as soon as it's allowed, causing orphans

[03-Aug-19 01:20 AM] blackjok3r#3181
🤔

[03-Aug-19 01:21 AM] zawy#5864
Or a big miner being blocked will go ahead and do a selfish mine, submitted each one as soon as he can.  He could mine 1000 blocks and then go some place else and mine, slowly submitting the 1000 when he can.

[03-Aug-19 01:23 AM] blackjok3r#3181
the minimum block time is a moving target?

[03-Aug-19 01:24 AM] zawy#5864
I don't follow.  Miner is supposed to assign an accurate timestamp or risk not getting accepted

[03-Aug-19 01:25 AM] blackjok3r#3181
yeah, dont think that works sorry. just thinking out loud

[03-Aug-19 02:48 AM] ComputerGenie 👴🏼📢#7004
`if blocks are going too fast why not reduce the block reward?`
because then you're dicking the "good" miner twice, just to "punish" the "bad" one

[03-Aug-19 04:07 AM] blackjok3r#3181
https://github.com/blackjok3rtt2/komodo/tree/randomblocktime
This is probably really stupid...

{Embed}
https://github.com/blackjok3rtt2/komodo/tree/randomblocktime
blackjok3rtt2/komodo
Komodo. Contribute to blackjok3rtt2/komodo development by creating an account on GitHub.
https://avatars1.githubusercontent.com/u/52269485?s=400&v=4

[03-Aug-19 04:07 AM] blackjok3r#3181
But I couldnt get it out of myt head and decided to see if it would work

[03-Aug-19 08:04 AM] jl777c#5810
@zawy i really like the three MA approach as this makes it impossible for attacker to just timestamp + 15 and bypass the single 4x trigger. not sure why you have 4, 7, 12, in the past i had good luck with prime based filters for noise reduction which makes it hard to find any pattern that isnt caught by them, 3, 5, 7, 11, 13, 17 would be the choices, but maybe it is all overkill and there is some better reason for 4, 7, 12. I implemented it as above and just have the exponential calc left to do. the way i implemented it, having 6 MA wont be any trouble.

one idea to make it symmetric is to use a max() on all the above and use it to reduce diff in the event recent blocks are too slow. so an overall trigger as to the direction and then choosing either min() or max() of the same MA values. I think that would make it symmetric, but then biased overall toward dropping due to the diff stranding handling. maybe that can be balanced out by making the max() side take into account one fewer MA. 

without modeling, it is just guesswork, but it seems that it would be pretty symmetric other than the diff stranding recovery

[03-Aug-19 08:05 AM] jl777c#5810
if you think the symmetric min/max has merit, I can enable that for todays test

[03-Aug-19 09:29 AM] jl777c#5810
the exponential is working much gentler than the N^2, as expected. however, i tried without a compare of the time for the last12 blocks < 11*T, and the problem was that it was still boosting diff from the earlier hashrate, it seems to take several blocks for the diff from stop increasing, even after the hashrate is gone. of course after passage of enough time, the exponential kicks in.
still it seems better to have an additional test of <11T before making it harder and i also added a >13T before making it easier; the latter should give it an extra kick to the exponential decay, which feels slow, but only in comparison to the N^2.

[03-Aug-19 09:32 AM] jl777c#5810
i have only tested with CPU so far, but it is responding much faster to even just having 12 cores mining as compared to before. and after reducing to 1 core, within 15 blocks, the diff comes down to where it is getting one block per minute. before, it took a long time before the blockrate was the target rate, so your magic MA is working fantastic!

[03-Aug-19 09:35 AM] jl777c#5810
i still havent seen the max() side kick in yet, likely will need ASIC testing to trigger that side. my prediction is that it will just enhance the exponential decay a bit. i think this assymettry of the downward adjustment having an exponential decay might create some opening for miner games, but without the max() side to push down the diff, it seems there is far more room for games as the exponential decay doesnt kick in without a very long block.

[03-Aug-19 09:39 AM] jl777c#5810
so currently, unless the last 12 blocks happened faster than 11T or longer than 13T, nothing happens, other than exponential decay that is triggered with any long gap. the exponential is always the highest priority. if most recent 12 blocks are <11T, the min of the MA and bnTarget is used, if >13T then the max is used. i see many cases where it is <11T but there is nothing smaller than bnTarget, so it doesnt look like it has much noise.

[03-Aug-19 09:40 AM] jl777c#5810
so far it feels quite responsive and adjusts very fast to any new hashrate, and also back down when the hashrate is removed

[03-Aug-19 10:08 AM] jl777c#5810
https://medium.com/@jameslee777/evolution-of-adaptivepow-dfea220d343f

{Embed}
https://medium.com/@jameslee777/evolution-of-adaptivepow-dfea220d343f
Evolution of AdaptivePoW
after my post last week, the acknowledge expert in the DAA (difficulty adjustment algorithms) zawy…

[03-Aug-19 10:19 AM] jl777c#5810
just did another test. diff was at 4 with one core, started 15 and in 18 minutes it got 21 blocks and the diff is now 20

[03-Aug-19 10:20 AM] jl777c#5810
to have only 3 extra blocks as the diff rises from 4 to 20, that seems pretty good, not sure how it would translate to an ASIC starting up, but it seems that CPU mining tests are all working quite well.

[03-Aug-19 10:35 AM] jl777c#5810
as expected the exponential decay kicked in to drop the diff from 22 down to 8, but it is still double what it should be and the >13T side is activated now reducing the diff a bit faster as the exponential decay is not activating

[03-Aug-19 10:49 AM] jl777c#5810
20 blocks in about 30 minutes diff 5.7, so not bad at all.

[03-Aug-19 12:07 PM] zawy#5864
As someone pointed out to me last year, a symmetrical DA function F(t/T) to make this correct is  integral(F(t/T)\*e^(t/T))  = 1    This will result in the correct average solvetime.   I'm trying to figure out how to merge that idea with  a max of several functions that trigger on the upper and lower bounds.  I'm also thinking adjusting reward in some way along with difficulty has the most potential.

[03-Aug-19 12:59 PM] dukeleto#7326
@zawy it's possible to make piece-wise defined functions "smooth" and differentiable

[03-Aug-19 01:00 PM] dukeleto#7326
maybe you can create a function which is that integral, except on certain points/ranges

[03-Aug-19 01:00 PM] dukeleto#7326
and make it smooth

[03-Aug-19 01:01 PM] dukeleto#7326
Adjusting reward sounds like it makes predicting the emission schedule hard, like if I wanted to know how much coins have been emitted at time/block X

[03-Aug-19 01:02 PM] dukeleto#7326
that integral definition of F(t), where is that from?

[03-Aug-19 01:02 PM] dukeleto#7326
I might have to bust out some diff-eq moves on it

[03-Aug-19 01:02 PM] dukeleto#7326
to confirm, that integral is `dt` and not `dT`, correct ?

[03-Aug-19 01:04 PM] dukeleto#7326
I have done some math on an index card

[03-Aug-19 01:04 PM] dukeleto#7326
just like old times

[03-Aug-19 01:05 PM] dukeleto#7326
the integral you state above, it also implies ` F' = -F`

[03-Aug-19 01:05 PM] dukeleto#7326
which means `F(x) = C1*e^(-k*x) + C2`

[03-Aug-19 01:06 PM] dukeleto#7326
all you said was "it must be an exponential", with that integral, but you also said something about those constants. I could tell you what they are in terms of T

[03-Aug-19 01:12 PM] jl777c#5810
changing reward is too big a change to the codebase to make it practical to implement

[03-Aug-19 01:13 PM] jl777c#5810
with ASIC test it took 22 minutes for 100 blocks to get to 15k diff, so todays is slower than yesterdays

[03-Aug-19 01:18 PM] zawy#5864
@jl777c I chose the 3 based on intuition.  most of the benefit is achieved by using any 1 of them.  The N=4 catches big attacks early, but it also catches smaller attacks by some luck.   Conversely, the N=12 catches smaller HR increases, but also catches big ones before 12 blocks.  Also, notice the divisor can get smaller as N goes from 4 to 7 to 12, so if an attack is sustained to >= 12 blocks, the  N=12 SMA gets the difficulty 1/2 way to where it needs to be.....in only 12 blocks, sort of.   The faster SMAs are to jump-start it. Being within 1/2 , I then left it up to the conservatism of Digishield to slowly get to where it needs to be, taking maybe another 60 blocks.  Primes are too frequent.   The 5, 3, and 2 divsors are as small as possible without causing false triggers during normal hashrate.....and not overshooting difficulty if it is a false positive.  I estimate the divisor (before testing to refine it for stability) with divisor = N / (N-2\*SQRT(N) )   This is based on the rough basic 95% probability that N samples will be within have 2 std devs of the true average.   Std Dev of N sample is roughly SQRT(N).  For this reason it's not about primes, and as you go to higher N you see less benefit.   More roughly, the idea is to nearly double N before you see noticeable improvement, hence 4, 7, 12..  Doing it again with  N=20 is feasible to help digishild be faster. The divisor would be about 1.5.

[03-Aug-19 01:18 PM] zawy#5864
To make this symmetrical following the SMA route, the following is what i'll do today:   For each N-window SMA find the 0.25% per block probability that the SMA would be triggered if hashrate is constant and difficulty is correct. This means I find the  lambda for a given N=k window until the probability is 0.0025\*N.  The divsor is k/lamba.    The divisor is > 1  as above when blocks are being solved too fast and < 1 when they are coming too slow.  In this way I believe we will have a  symmetrical  and  very fast response.  I believe it will require only N=1, 2, 4, 8, 16, and 32 windowns for fast and slow block times (12 total).  So the  highest-level of the  DA I think would be:
```
bnTarget = min( fast-solve SMAs , max( slow-solve SMAs, digishield));
```

[03-Aug-19 01:28 PM] jl777c#5810
makes sense about the MA sizes. i used the same MA sums for both directions, just used min vs max to change direction. anyway looking forward to the ideal fast/slow MA parameters. in the meantime, we will test adding the 4x < T in addition to the current symmetric to try to get it to respond faster to the rise, the reduction in hashrate was slower than N^2, but still a lot faster than the ramp up

[03-Aug-19 01:29 PM] zawy#5864
@dukeleto The integral is d(t/T).   That equation shows that F(t/T) might be a lot more analyzable if it is of the e^x form instead of trying to use SMAs.

[03-Aug-19 01:30 PM] jl777c#5810
i know this is a "frankenstein" hybrid nowhere as elegant as the fast/slow symmetrical, but experimentally i think it can do quite well

[03-Aug-19 01:35 PM] jl777c#5810
also for the fast/slow, even if a MA is needed for every N, that is ok, the overhead wont be large at all

