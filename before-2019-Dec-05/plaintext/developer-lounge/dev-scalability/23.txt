==============================================================
Guild: Komodo
Channel: dev-scalability
Topic: Discussion about Komodo scalability. Komodo has demonstrated 20,000 TPS.
Messages: 100
Range: before 05-Dec-19 12:00 AM
==============================================================

[07-Jul-18 09:53 PM] John the Cashist#7046
We need a crypto olympics ü§îü§î

[08-Jul-18 09:27 PM] Deleted User#0000
didn't check this chan for a few time, in fact the script can be ran on any server / instance, the only tricky part would be just to extract each chain own table, and just run the counting for the global table. Assuming it has been done with hundreds of chains without harm, as long as we're able to get each chain's table, it's only a few commands to import em on same db instance

[08-Jul-18 09:27 PM] Deleted User#0000
and just launch the "globalizing" method

[08-Jul-18 09:34 PM] Deleted User#0000
the db for tech users just wouldn't be a proof, I think, launching the chains and verifying blocks would be a solution, to see peaks, it can also be combined with the db/webinterface to query automagically the corresponding blocks (for example just a little script with the db at a timestamp could return the exact block involved in the peak, then could be used in a loop to check on every chain the number of transactions)

[08-Jul-18 09:36 PM] Deleted User#0000
that PoV would require being on CLI. The only method for users not using CLI to check it would be explorers, but I think it was said that it was a pain in term of hardware required. Just in case it is needed, it's the same as the CLI : It's totally possible to "link" the peak to each specific explorer URL to count via GUI

[09-Jul-18 10:17 AM] benohanlon#1119
thanks @sœÜldat I'm on it @jl777c now

[09-Jul-18 10:18 AM] benohanlon#1119
- posting in their telegram and emailing them

{Reactions}
shexy (2) 

[10-Jul-18 08:21 AM] blackjok3r#3181
When you fire up AWS to run a scale test, and its still not working üò´

[10-Jul-18 08:36 AM] Mylo#8306
d'oh.  which part isn't working?

[10-Jul-18 08:45 AM] blackjok3r#3181
We have EC2, but its locked down to tiny instances. Cant launch anything meaningful in size. I applied for increase, but if past encounters with support are anything to go by, it could take a while.

[10-Jul-18 08:46 AM] blackjok3r#3181
EG I can apply for an m5.large instance... but I need an m5.x16large lol...

[10-Jul-18 08:46 AM] blackjok3r#3181
so do we need to increase limits one teir at a time? or will they just upgrade directly to the top one is the question lol.

[10-Jul-18 09:05 AM] blackjok3r#3181
Lucky we still have @Mylo 's account. üòÑ

[10-Jul-18 09:06 AM] blackjok3r#3181
I am rebuilding my docker image using komodo DEV branch, rather than momo as MoMoM was merged into that a while back.

[10-Jul-18 09:06 AM] blackjok3r#3181
ü§û everything is working in sync now.

{Reactions}
üí™ 

[11-Jul-18 11:27 PM] anonanon#1085
Say I'm concerned the recent 51% attacks on zencash and BTG will be repeated on lots of cryptocurrencies that share common mining algorithms. I figure Komodo's dPOW is intended to protect against that, but can anyone tell me what it would look like if someone rented 51% of the KMD hashpower and tried to perform a double spend or otherwise mess with the chain?

[11-Jul-18 11:28 PM] jl777c#5810
notaries mine 75% of blocks

[11-Jul-18 11:28 PM] jl777c#5810
so 51% of 25% is about 13% of effective hashrate

[11-Jul-18 11:29 PM] jl777c#5810
KMD has some unique defenses against 51%

[11-Jul-18 11:29 PM] jl777c#5810
in addition to dPoW notarization

[11-Jul-18 11:29 PM] jl777c#5810
which stops cold any attempt at double spends (up to the notarized height)

[11-Jul-18 11:30 PM] jl777c#5810
in other words, you cant rent 51% of KMD hashpower as 75% of the effective hashrate is with the notaries

[11-Jul-18 11:30 PM] anonanon#1085
Right, nice. Ty!

[11-Jul-18 11:40 PM] anonanon#1085
Oh I'm not sure how I ended up in scaling-test. My bad y'all.

[12-Jul-18 09:58 AM] blackjok3r#3181
OK guys...

[12-Jul-18 09:58 AM] blackjok3r#3181
http://cryptocartography.io/txscl_vis/

[12-Jul-18 10:01 AM] blackjok3r#3181
The TX and Payments per second reported live and confirmed to be 90% or more accurate. No mean feat I assure you. This is still only 64 chains and will be one of the last "real" tests we run, before moving to a "block simulator" to test the site at very large scales of chains. 

The TxBlaster itself is proving very reliable and there is no longer a reason to suck up AWS credits to run full blockchain tests. So we will move to a randomly generated stats model, and keep doubling the number of chains until the site breaks. We can separate out the back end of the website into multiple streams to scale to any number. Just like the TxBlaster's which can scale to infinity given enough CPU's .

[12-Jul-18 10:03 AM] blackjok3r#3181
If anyone is around to watch the site in the next hour or so please let us know if there is anything weird going on. 
known issues:
1) the Blocks per minute number jumps around a bit, we might need to apply a soothing functon to this data, blocks come in chunks, and then breaks with no blocks. Its just how blockchains work.
2) Refreshing the website, which moves it to 'historical data' causes the reported data to be incorrect. This is being worked on, and is not anything major. Live data is correct.

[12-Jul-18 10:04 AM] blackjok3r#3181
https://komodostats.com/scaling/index2.php?all

[12-Jul-18 10:05 AM] blackjok3r#3181
That is another data source from out test last night that we used to verify the site is right. Approx 9000 - 10,000 TX/s peak and 7500 Tx/s sustained.
61,000 payments per second. peak... 
55,000 PPS sustained.

{Reactions}
üëè (2) 

[12-Jul-18 10:07 AM] blackjok3r#3181
üöÄ

[12-Jul-18 12:56 PM] blackjok3r#3181
https://youtu.be/kXk7Y3Si140

{Embed}
Good onya Mate
https://youtu.be/kXk7Y3Si140
ScaleTest 64 Chains
Komodo Scaling test 64 Chains
https://i.ytimg.com/vi/kXk7Y3Si140/maxresdefault.jpg

[12-Jul-18 12:58 PM] blackjok3r#3181
Short summery of the last test. The https://komodostats.com/scaling/index2.php will be updated with the results of this test when webworker is available.

[12-Jul-18 01:19 PM] Deleted User#0000
Mmm grass fed beef üç¥

[12-Jul-18 01:23 PM] blackjok3r#3181
lol link smk762 gave me yesterday... forgot it was open lol

[12-Jul-18 01:24 PM] blackjok3r#3181
I mostly did the video to record the data, as the site breaks when you refresh it atm.

[13-Jul-18 02:47 AM] blackjok3r#3181
@webworker01 any chance you can change the SMA to 5 mins... as thats what smk's site uses... that might give me a better way to compare data.

[13-Jul-18 02:47 AM] blackjok3r#3181
I think te raw data is much the same, but the way its smoothed out is diffrent so gives diffrent numbers

[13-Jul-18 03:35 AM] blackjok3r#3181
yeah thats fine. It keeps both seperated... if you look at the video of the stats site, smk seperates them, with two diffrent graph colours.

[14-Jul-18 03:22 AM] smk762 -  dracocanis ominator#7640
Vis history smoothing has been fixed. Had a small issue where some blocks were not being captured on the vis, leading to underreporting of transactions/payments.The fix that made sure no blocks were dropping involved recording values in 5 second increments, with increments aligned to be a unix timestamp which is a factor of 5.
It wasn't really the smoothing algo that was the problem, it was the way history was being recorded and populating the graph.

[14-Jul-18 03:25 AM] smk762 -  dracocanis ominator#7640
The fix to make sure all data was being recieved live resulted in blocks that were arriving late being recorded, but created potentially 3 or 4 database entries with the same timestamp. Instead of the sum of these being emitted, only the first recieved was, meaning even though blocks were being recieved and recorded, and reporting true with live data, the history data from after the initial fix wasn't passing all the info on to the highcharts graph.

[14-Jul-18 03:27 AM] smk762 -  dracocanis ominator#7640
edits were made last night which ensured all source data recieved and stored in history are passed in full to the graph.

[14-Jul-18 03:28 AM] smk762 -  dracocanis ominator#7640
A note has also been added to explain the minor differences between the vis graph and komodostats.com

[14-Jul-18 03:29 AM] smk762 -  dracocanis ominator#7640
*Values represented in this visualisation are plotted at 5 second increments, at intervals of 1 and 5 minutes. The finer resolution of graph points per minute, and the slight delay in recieving and processing block data, results in minor variations to values compared to the source block data graph at komodostats.com, whick are plotted at 1 and 5 minute resolution. The source block data has been stored for verification, and is normally plotted at komodostats.com within 24hrs after each test.*
*Comparison between source block data and aggregate data used for this visualisation are reviewed after each test, with validated test periods highlighted with a light green background.*
*Random data which has been used for testing is purged periodically.*

{Reactions}
üí™ (4) ü•â (3) 

[14-Jul-18 03:54 AM] smk762 -  dracocanis ominator#7640
we're now on track to add more chains, and find out where that saturates the database and 5 second increment. From there, we should be able to find out what increment is best for 8192 chains, make some edits to widen it accordingly, and march on to 1million tx üòÉ

{Reactions}
üî• (3) 

[14-Jul-18 03:58 AM] blackjok3r#3181
Will work on a block simulator this afternoon, see if we can fill the DynamoDB with 8192 chains üòÑ

[14-Jul-18 04:23 AM] smk762 -  dracocanis ominator#7640
https://i.imgur.com/hscsCQJ.jpg

{Embed}
https://imgur.com/hscsCQJ
https://i.imgur.com/hscsCQJ.jpg

{Reactions}
‚ö° 

[14-Jul-18 12:14 PM] blackjok3r#3181
https://i.imgur.com/DqrP09z.png

{Embed}
https://imgur.com/DqrP09z
Screenshot
https://i.imgur.com/DqrP09z.png

[14-Jul-18 12:14 PM] blackjok3r#3181
üòÇ

[14-Jul-18 12:15 PM] blackjok3r#3181
Using simulated data rather than actual blockchains, but its working perfectly.

[14-Jul-18 12:15 PM] blackjok3r#3181
Does anyone have any requests of a number we should aim for using simulated data? I think maybe we can make it pay everyone in the entire world in 30s without too much effort ü§£

[14-Jul-18 12:18 PM] jl777c#5810
pretty crazy!

[14-Jul-18 01:15 PM] blackjok3r#3181
About to start a simulated 4096 chains test. üöÄ

[14-Jul-18 01:17 PM] blackjok3r#3181
Simulating works by sending the JSON with a the data of a full block at random intervals between 30-90s to the AWS DynamoDB. Saves a huge amount of CPU's and  storage space... but really is not much different to using a real blockchain to generate the data.

[14-Jul-18 01:18 PM] blackjok3r#3181
once the stats are verified to work with X chains, then we are ready. All we need is AWS to allow out accout to launch X vCPU's at once. We can even use multiple regions if need be.

[14-Jul-18 01:19 PM] blackjok3r#3181
Or even multiple accounts if it comes down to that.

{Reactions}
üëå shexy üëç 

[14-Jul-18 03:24 PM] blackjok3r#3181
ü§£

[14-Jul-18 03:24 PM] blackjok3r#3181
Redlined the speedo

[14-Jul-18 03:24 PM] blackjok3r#3181
http://cryptocartography.io/txscl_vis/

[14-Jul-18 03:28 PM] blackjok3r#3181
https://youtu.be/pVEx1Hg_I7I

{Embed}
Good onya Mate
https://youtu.be/pVEx1Hg_I7I
video2
https://i.ytimg.com/vi/pVEx1Hg_I7I/maxresdefault.jpg

[15-Jul-18 12:58 AM] Mylo#8306
That's awesome!! So when are we going to be doing some of these crazy tests?  Will it be in the next:
 a) 1 -3 days
 b) 3-5 days
 c) 7-10 days
 d) ask tomorrow

üòÉ

[15-Jul-18 01:23 AM] blackjok3r#3181
That's not known at this stage, I await patchkez to return from vacation to help me with the last of the docker stuff, need to create a kubernetes template, and also Steve has to sort the last things out with AWS, so we have an account big enough to launch the full test. Some time in the next week I will run a full block chain test with real data that aims for 45k tx/s minimum to test out my docker containers running in kubernetes to verify everything works. Also need to get some notary nodes and explorer's for the first 64 chains ready. Maybe I can just run these in AWS as well, but in a different region. Not sure yet.

{Reactions}
üëå 

[15-Jul-18 03:36 AM] smk762 -  dracocanis ominator#7640
Ive added a couple of things to the graph point tooltips - commas for number formatting, active chain count, and data state (live / simulated).

{Reactions}
üëç 

[15-Jul-18 03:36 AM] smk762 -  dracocanis ominator#7640
https://i.imgur.com/vkulxpX.jpg

{Embed}
https://imgur.com/vkulxpX
https://i.imgur.com/vkulxpX.jpg

[15-Jul-18 04:08 AM] smk762 -  dracocanis ominator#7640
Is there a plan to host the source data at a publicly available url for validation?  I understand it is a lot of data... but I'd like to add a link for the curious. Maybe a link to contact details to someone which can provide it on request for a specific time period?

[15-Jul-18 04:35 AM] blackjok3r#3181
I think the blockchains will be saved in an AWS S3 data store, however running 8192 deamons to extract it would be difficult. I will work with dwy to adapt his script that dumps the data to a mysql DB. Maybe that can be put out to public for reads?

[15-Jul-18 05:01 AM] smk762 -  dracocanis ominator#7640
Third party verification from an independent, trusted person/group is probably most valuable marketing-wise, but opening it to the public also shows good faith. Most wouldn't bother to check, but availability speaks volumes. Not sure what format is best. Database tables are easily populated with whatever, but a full copy of all chains blk.dat etc is closer to source, though a significant amount of data to review. Probably a few random samples would suffice.
The scripts which are used to run the tests being open source also helps, at least for those that understand it. I don't think we'll have issues with burden of proof if the right people with no vested interest confirm it's valididty.

[15-Jul-18 05:02 AM] smk762 -  dracocanis ominator#7640
sql dumps are small enough for public consumption and a good indicator of valididty. Official validation would likely want access to more

[15-Jul-18 05:03 AM] smk762 -  dracocanis ominator#7640
I think that expanding the notes link on the vis site to be a pop-up onclick with links to sql and all relevant github repos isnt a bad idea

[15-Jul-18 05:06 AM] smk762 -  dracocanis ominator#7640
something like this, if it allows importing blk.dat files, would be handy - http://yogh.io/#block:last

[16-Jul-18 10:59 PM] Deleted User#0000
btw, maybe you can try this to test stuff : https://labs.ovh.com/kubernetes-k8s

[16-Jul-18 10:59 PM] Deleted User#0000
seems they provide alpha phase totally free

[16-Jul-18 11:00 PM] Deleted User#0000
don t know if that's stable, but for testing purposes maybe it's worth trying

[17-Jul-18 01:48 AM] polycryptoblog#1173
3rd party trusted verification you say,  hmm may have the guys for that.

{Reactions}
üõ∞ (2) 

[17-Jul-18 01:52 AM] blackjok3r#3181
Yeah that would a good, if we can set up something like that. We are looking at an estimated 6-7TB of blockchain data generated in the test, but depending on cost, it could be much larger as I would prefer to make the test run longer than that if possible.

[17-Jul-18 02:17 AM] blackjok3r#3181
Btw, thanks @Deleted User  I signed up for that OVH Alpha test, doubtful they have anything near the needed amount of CPU's but the way the test works, we can spread over loads of providers if need be.

{Reactions}
üòÉ 

[17-Jul-18 07:46 AM] Mylo#8306

{Attachments}
https://cdn.discordapp.com/attachments/449949868904022036/468685021880778773/Metric_Definition_Proposal.pdf

[17-Jul-18 07:48 AM] Mylo#8306
This is a PDF print of a google doc shared by hyperledger.
Link from the working group home page (WG Working Documents) here https://wiki.hyperledger.org/groups/pswg/performance-and-scale-wg

[17-Jul-18 08:04 AM] Mylo#8306
From the last test @blackjok3r @webworker01 @smk762 -  dracocanis ominator - is it true to say:
```
 - Live test to optimize using a smaller load
 - 6k tx/s for 15 mins
 - 4k tx/s for 30 mins
 - 2k tx/s for 60 mins
 - details: https://komodostats.com/scaling/index2.php?all
```
?

[17-Jul-18 08:06 AM] blackjok3r#3181
It depends what you are trying to say... I can launch a test with 256 chains that cna run for 24H if thats a better meteric to use?

[17-Jul-18 08:06 AM] blackjok3r#3181
Maybe we should run some smaller scale tests for extended time periods, then a really large amount of chains for short time?

[17-Jul-18 08:07 AM] blackjok3r#3181
then we can prove that its technically possible to sustain very large numbers at scale... Its only a cost factor that limits the time.

[17-Jul-18 08:07 AM] Mylo#8306
Yes, can we run a small scale for 12 hours doing 2k tx/s with stats working the whole time?
What is the cost difference between 2k, 4k, 8k tx/s for the purpose of this test?

[17-Jul-18 08:09 AM] blackjok3r#3181
64 chains seems to cap out at about 7000tx/s of single payment, but sustaining that for long periods is difficult... if you increase it out to 3-4 payment TX's it should be possible to run indefinitely.

[17-Jul-18 08:09 AM] blackjok3r#3181
Or I can go back to an earlier iteration, that uses 2 TxBlasters to blast for unlimited time.

[17-Jul-18 08:09 AM] blackjok3r#3181
It was just not possible to use that version at very large scale of chains, dude to limits of hardware available.

[17-Jul-18 08:10 AM] Mylo#8306
Is there a bottleneck with the single payment/

[17-Jul-18 08:11 AM] blackjok3r#3181
yes on the transaction generating side... it requires 2 marketmakers/komodods to broadcast enough transactions to fill the mempool.

[17-Jul-18 08:11 AM] blackjok3r#3181
When you are using one wallet to simulate over 1000 wallets, its a little hard.

[17-Jul-18 08:11 AM] blackjok3r#3181
If we had 1000 wallets to send TX from.... then it would not matter

[17-Jul-18 08:12 AM] blackjok3r#3181
Once again, limited by hardware.

[17-Jul-18 08:12 AM] blackjok3r#3181
Short of actually using it for a national currency, its all but impossible to get that many users.

[17-Jul-18 08:17 AM] Mylo#8306
so 64 chains doing single payment ~= 7k tx/s
doing 5 payments  allows for marketmaker to keep up with blasting to fill mempool quick enough by sending transctions containing more bytes but sustainable for longer.  Load testing endurance rather than throughput.
?

Users will be machines and dApps.

[17-Jul-18 08:19 AM] blackjok3r#3181
Yes, I have not actually tested a threshold for payments per TX that allows sustained full blocks, but 100 needs nothing, 10 chains will be blasting at any one time over 64 chains. but 1 payment is not really possible to keep mempool saturated. For this I used a mining pause at the start to fill all the mempools up.

[17-Jul-18 08:20 AM] blackjok3r#3181
I can definatly do that, although like I said there is an older version, that can sustain single payments, it just needs an extra CPU per chain.

[17-Jul-18 08:20 AM] Mylo#8306
Nah let's not use older version imo.  Endurance is another metric that will need to be known once throughput is grokked.

[17-Jul-18 08:21 AM] blackjok3r#3181
Not really, because the transactions being generated in this way do not apply to any real world use... they are a simulation.

[17-Jul-18 08:21 AM] blackjok3r#3181
What is being tested is the amount of confirmed TX that is possible.

[17-Jul-18 08:23 AM] blackjok3r#3181
Testing the TxBlaster API over endurance is pointless, and I think this is why James said a 15 minute window of all chains with full blocks was all that was required.. I did have a single chain locally runnign for over 5H with full blocks though, so it can be run for a long time

[17-Jul-18 08:25 AM] Mylo#8306
OK - very specific goal.  Got it.

